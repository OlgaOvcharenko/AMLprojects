{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pyod\n!pip install dataheroes \n!pip install lightgbm\n!pip install cubist\n!pip install catboost\n\n#\n# !pip install preprocess","metadata":{"execution":{"iopub.status.busy":"2023-11-10T13:40:01.167575Z","iopub.execute_input":"2023-11-10T13:40:01.168021Z","iopub.status.idle":"2023-11-10T13:41:20.504033Z","shell.execute_reply.started":"2023-11-10T13:40:01.167972Z","shell.execute_reply":"2023-11-10T13:41:20.502932Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting pyod\n  Downloading pyod-1.1.1.tar.gz (159 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.4/159.4 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from pyod) (1.3.2)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from pyod) (3.7.2)\nRequirement already satisfied: numpy>=1.19 in /opt/conda/lib/python3.10/site-packages (from pyod) (1.23.5)\nRequirement already satisfied: numba>=0.51 in /opt/conda/lib/python3.10/site-packages (from pyod) (0.57.1)\nRequirement already satisfied: scipy>=1.5.1 in /opt/conda/lib/python3.10/site-packages (from pyod) (1.11.2)\nRequirement already satisfied: scikit_learn>=0.22.0 in /opt/conda/lib/python3.10/site-packages (from pyod) (1.2.2)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from pyod) (1.16.0)\nRequirement already satisfied: llvmlite<0.41,>=0.40.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba>=0.51->pyod) (0.40.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit_learn>=0.22.0->pyod) (3.1.0)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->pyod) (1.1.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->pyod) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->pyod) (4.40.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->pyod) (1.4.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->pyod) (21.3)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->pyod) (9.5.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->pyod) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib->pyod) (2.8.2)\nBuilding wheels for collected packages: pyod\n  Building wheel for pyod (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pyod: filename=pyod-1.1.1-py3-none-any.whl size=190078 sha256=2e40f908cd5c85fb9dba19427992a82dfa1b998348b5f18bc77a9e3e5a4356d8\n  Stored in directory: /root/.cache/pip/wheels/a3/42/d7/48a53ffc1466bd63932f28583c64ebf442114db14a0bfa8c95\nSuccessfully built pyod\nInstalling collected packages: pyod\nSuccessfully installed pyod-1.1.1\nCollecting dataheroes\n  Downloading dataheroes-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.19.0 in /opt/conda/lib/python3.10/site-packages (from dataheroes) (1.23.5)\nRequirement already satisfied: scipy>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from dataheroes) (1.11.2)\nRequirement already satisfied: scikit-learn>=0.24.0 in /opt/conda/lib/python3.10/site-packages (from dataheroes) (1.2.2)\nRequirement already satisfied: pandas>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from dataheroes) (2.0.3)\nRequirement already satisfied: joblib>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from dataheroes) (1.3.2)\nRequirement already satisfied: threadpoolctl>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from dataheroes) (3.1.0)\nRequirement already satisfied: networkx>=2.5 in /opt/conda/lib/python3.10/site-packages (from dataheroes) (3.1)\nRequirement already satisfied: pydot>=1.4.1 in /opt/conda/lib/python3.10/site-packages (from dataheroes) (1.4.2)\nRequirement already satisfied: matplotlib>=3.3.0 in /opt/conda/lib/python3.10/site-packages (from dataheroes) (3.7.2)\nRequirement already satisfied: psutil>=5.8.0 in /opt/conda/lib/python3.10/site-packages (from dataheroes) (5.9.3)\nRequirement already satisfied: tables>=3.6.1 in /opt/conda/lib/python3.10/site-packages (from dataheroes) (3.8.0)\nRequirement already satisfied: decorator>=5.1.1 in /opt/conda/lib/python3.10/site-packages (from dataheroes) (5.1.1)\nRequirement already satisfied: opentelemetry-sdk>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from dataheroes) (1.18.0)\nRequirement already satisfied: opentelemetry-api>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from dataheroes) (1.18.0)\nRequirement already satisfied: opentelemetry-exporter-otlp>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from dataheroes) (1.18.0)\nCollecting licensing>=0.31 (from dataheroes)\n  Downloading licensing-0.41.tar.gz (11 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->dataheroes) (1.1.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->dataheroes) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->dataheroes) (4.40.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->dataheroes) (1.4.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->dataheroes) (21.3)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->dataheroes) (9.5.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->dataheroes) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->dataheroes) (2.8.2)\nRequirement already satisfied: deprecated>=1.2.6 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-api>=1.14.0->dataheroes) (1.2.14)\nCollecting importlib-metadata~=6.0.0 (from opentelemetry-api>=1.14.0->dataheroes)\n  Downloading importlib_metadata-6.0.1-py3-none-any.whl (21 kB)\nRequirement already satisfied: setuptools>=16.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-api>=1.14.0->dataheroes) (68.0.0)\nRequirement already satisfied: opentelemetry-exporter-otlp-proto-grpc==1.18.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-exporter-otlp>=1.14.0->dataheroes) (1.18.0)\nRequirement already satisfied: opentelemetry-exporter-otlp-proto-http==1.18.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-exporter-otlp>=1.14.0->dataheroes) (1.18.0)\nRequirement already satisfied: backoff<3.0.0,>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc==1.18.0->opentelemetry-exporter-otlp>=1.14.0->dataheroes) (2.2.1)\nRequirement already satisfied: googleapis-common-protos~=1.52 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc==1.18.0->opentelemetry-exporter-otlp>=1.14.0->dataheroes) (1.59.1)\nRequirement already satisfied: grpcio<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc==1.18.0->opentelemetry-exporter-otlp>=1.14.0->dataheroes) (1.51.3)\nRequirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.18.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc==1.18.0->opentelemetry-exporter-otlp>=1.14.0->dataheroes) (1.18.0)\nRequirement already satisfied: opentelemetry-proto==1.18.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc==1.18.0->opentelemetry-exporter-otlp>=1.14.0->dataheroes) (1.18.0)\nRequirement already satisfied: requests~=2.7 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-http==1.18.0->opentelemetry-exporter-otlp>=1.14.0->dataheroes) (2.31.0)\nRequirement already satisfied: protobuf<5.0,>=3.19 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-proto==1.18.0->opentelemetry-exporter-otlp-proto-grpc==1.18.0->opentelemetry-exporter-otlp>=1.14.0->dataheroes) (3.20.3)\nRequirement already satisfied: opentelemetry-semantic-conventions==0.39b0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-sdk>=1.14.0->dataheroes) (0.39b0)\nRequirement already satisfied: typing-extensions>=3.7.4 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-sdk>=1.14.0->dataheroes) (4.6.3)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.0->dataheroes) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.0->dataheroes) (2023.3)\nRequirement already satisfied: cython>=0.29.21 in /opt/conda/lib/python3.10/site-packages (from tables>=3.6.1->dataheroes) (0.29.35)\nRequirement already satisfied: numexpr>=2.6.2 in /opt/conda/lib/python3.10/site-packages (from tables>=3.6.1->dataheroes) (2.8.5)\nRequirement already satisfied: blosc2~=2.0.0 in /opt/conda/lib/python3.10/site-packages (from tables>=3.6.1->dataheroes) (2.0.0)\nRequirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.10/site-packages (from tables>=3.6.1->dataheroes) (9.0.0)\nRequirement already satisfied: msgpack in /opt/conda/lib/python3.10/site-packages (from blosc2~=2.0.0->tables>=3.6.1->dataheroes) (1.0.5)\nRequirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.10/site-packages (from deprecated>=1.2.6->opentelemetry-api>=1.14.0->dataheroes) (1.14.1)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata~=6.0.0->opentelemetry-api>=1.14.0->dataheroes) (3.15.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->dataheroes) (1.16.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests~=2.7->opentelemetry-exporter-otlp-proto-http==1.18.0->opentelemetry-exporter-otlp>=1.14.0->dataheroes) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests~=2.7->opentelemetry-exporter-otlp-proto-http==1.18.0->opentelemetry-exporter-otlp>=1.14.0->dataheroes) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests~=2.7->opentelemetry-exporter-otlp-proto-http==1.18.0->opentelemetry-exporter-otlp>=1.14.0->dataheroes) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests~=2.7->opentelemetry-exporter-otlp-proto-http==1.18.0->opentelemetry-exporter-otlp>=1.14.0->dataheroes) (2023.7.22)\nBuilding wheels for collected packages: licensing\n  Building wheel for licensing (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for licensing: filename=licensing-0.41-py3-none-any.whl size=13052 sha256=2346bcf652f6bebd18d2975a3bef4e7394f164b00121a1619b56086fa11872dc\n  Stored in directory: /root/.cache/pip/wheels/f4/48/bb/b1b19bd137eab8995aa23393709fc223e9418ab21bba0836b1\nSuccessfully built licensing\nInstalling collected packages: licensing, importlib-metadata, dataheroes\n  Attempting uninstall: importlib-metadata\n    Found existing installation: importlib-metadata 6.7.0\n    Uninstalling importlib-metadata-6.7.0:\n      Successfully uninstalled importlib-metadata-6.7.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncmudict 1.0.13 requires importlib-metadata<6.0.0,>=5.1.0, but you have importlib-metadata 6.0.1 which is incompatible.\nyapf 0.40.1 requires importlib-metadata>=6.6.0, but you have importlib-metadata 6.0.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed dataheroes-0.7.0 importlib-metadata-6.0.1 licensing-0.41\nRequirement already satisfied: lightgbm in /opt/conda/lib/python3.10/site-packages (3.3.2)\nRequirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from lightgbm) (0.40.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from lightgbm) (1.23.5)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from lightgbm) (1.11.2)\nRequirement already satisfied: scikit-learn!=0.22.0 in /opt/conda/lib/python3.10/site-packages (from lightgbm) (1.2.2)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn!=0.22.0->lightgbm) (1.3.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn!=0.22.0->lightgbm) (3.1.0)\nCollecting cubist\n  Downloading cubist-0.1.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (496 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m497.0/497.0 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from cubist) (1.23.5)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from cubist) (2.0.3)\nRequirement already satisfied: scikit-learn>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from cubist) (1.2.2)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.23.0->cubist) (1.11.2)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.23.0->cubist) (1.3.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.23.0->cubist) (3.1.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->cubist) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->cubist) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->cubist) (2023.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->cubist) (1.16.0)\nInstalling collected packages: cubist\nSuccessfully installed cubist-0.1.0\nRequirement already satisfied: catboost in /opt/conda/lib/python3.10/site-packages (1.2.1)\nRequirement already satisfied: graphviz in /opt/conda/lib/python3.10/site-packages (from catboost) (0.20.1)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from catboost) (3.7.2)\nRequirement already satisfied: numpy>=1.16.0 in /opt/conda/lib/python3.10/site-packages (from catboost) (1.23.5)\nRequirement already satisfied: pandas>=0.24 in /opt/conda/lib/python3.10/site-packages (from catboost) (2.0.3)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from catboost) (1.11.2)\nRequirement already satisfied: plotly in /opt/conda/lib/python3.10/site-packages (from catboost) (5.15.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from catboost) (1.16.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.24->catboost) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.24->catboost) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.24->catboost) (2023.3)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->catboost) (1.1.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->catboost) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->catboost) (4.40.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->catboost) (1.4.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->catboost) (21.3)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->catboost) (9.5.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->catboost) (3.0.9)\nRequirement already satisfied: tenacity>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from plotly->catboost) (8.2.2)\n","output_type":"stream"}]},{"cell_type":"code","source":"import cubist\nfrom catboost import CatBoostRegressor\nimport numpy as np\nimport umap\nfrom mlxtend.plotting.pca_correlation_graph import corr2_coeff\nfrom pyod.models.ecod import ECOD\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import BayesianRidge\nfrom sklearn.preprocessing import RobustScaler, MinMaxScaler,StandardScaler\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.feature_selection import f_regression, SelectKBest, chi2, VarianceThreshold\nimport numpy as np\nfrom lightgbm import LGBMRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor, AdaBoostRegressor, StackingRegressor, RandomForestRegressor\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import RationalQuadratic\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.linear_model import RidgeCV, Lasso, ElasticNet, LinearRegression\nfrom sklearn.model_selection import KFold, GridSearchCV\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import LinearSVR, SVR\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import KFold\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.impute import KNNImputer\nfrom sklearn.svm import LinearSVR, SVR\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import r2_score\nfrom dataheroes import CoresetTreeServiceDTC\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-11-10T13:41:20.506083Z","iopub.execute_input":"2023-11-10T13:41:20.506545Z","iopub.status.idle":"2023-11-10T13:41:55.246673Z","shell.execute_reply.started":"2023-11-10T13:41:20.506501Z","shell.execute_reply":"2023-11-10T13:41:55.245311Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n  @numba.jit()\n/opt/conda/lib/python3.10/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n  @numba.jit()\n/opt/conda/lib/python3.10/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n  @numba.jit()\n/opt/conda/lib/python3.10/site-packages/umap/umap_.py:660: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n  @numba.jit()\n","output_type":"stream"}]},{"cell_type":"code","source":"\"\"\"Relevance Vector Machine classes for regression and classification.\"\"\"\nimport numpy as np\n\nfrom scipy.optimize import minimize\nfrom scipy.special import expit\n\nfrom sklearn.base import BaseEstimator, RegressorMixin, ClassifierMixin\nfrom sklearn.metrics.pairwise import (\n    linear_kernel,\n    rbf_kernel,\n    polynomial_kernel\n)\nfrom sklearn.multiclass import OneVsOneClassifier\nfrom sklearn.utils.validation import check_X_y\n\n\nclass BaseRVM(BaseEstimator):\n\n    \"\"\"Base Relevance Vector Machine class.\n\n    Implementation of Mike Tipping's Relevance Vector Machine using the\n    scikit-learn API. Add a posterior over weights method and a predict\n    in subclass to use for classification or regression.\n    \"\"\"\n\n    def __init__(\n        self,\n        kernel='rbf',\n        degree=3,\n        coef1=None,\n        coef0=0.0,\n        n_iter=3000,\n        tol=1e-3,\n        alpha=1e-6,\n        threshold_alpha=1e9,\n        beta=1.e-6,\n        beta_fixed=False,\n        bias_used=True,\n        verbose=False\n    ):\n        \"\"\"Copy params to object properties, no validation.\"\"\"\n        self.kernel = kernel\n        self.degree = degree\n        self.coef1 = coef1\n        self.coef0 = coef0\n        self.n_iter = n_iter\n        self.tol = tol\n        self.alpha = alpha\n        self.threshold_alpha = threshold_alpha\n        self.beta = beta\n        self.beta_fixed = beta_fixed\n        self.bias_used = bias_used\n        self.verbose = verbose\n\n    def get_params(self, deep=True):\n        \"\"\"Return parameters as a dictionary.\"\"\"\n        params = {\n            'kernel': self.kernel,\n            'degree': self.degree,\n            'coef1': self.coef1,\n            'coef0': self.coef0,\n            'n_iter': self.n_iter,\n            'tol': self.tol,\n            'alpha': self.alpha,\n            'threshold_alpha': self.threshold_alpha,\n            'beta': self.beta,\n            'beta_fixed': self.beta_fixed,\n            'bias_used': self.bias_used,\n            'verbose': self.verbose\n        }\n        return params\n\n    def set_params(self, **parameters):\n        \"\"\"Set parameters using kwargs.\"\"\"\n        for parameter, value in parameters.items():\n            setattr(self, parameter, value)\n        return self\n\n    def _apply_kernel(self, x, y):\n        \"\"\"Apply the selected kernel function to the data.\"\"\"\n        if self.kernel == 'linear':\n            phi = linear_kernel(x, y)\n        elif self.kernel == 'rbf':\n            phi = rbf_kernel(x, y, self.coef1)\n        elif self.kernel == 'poly':\n            phi = polynomial_kernel(x, y, self.degree, self.coef1, self.coef0)\n        elif callable(self.kernel):\n            phi = self.kernel(x, y)\n            if len(phi.shape) != 2:\n                raise ValueError(\n                    \"Custom kernel function did not return 2D matrix\"\n                )\n            if phi.shape[0] != x.shape[0]:\n                raise ValueError(\n                    \"Custom kernel function did not return matrix with rows\"\n                    \" equal to number of data points.\"\"\"\n                )\n        else:\n            raise ValueError(\"Kernel selection is invalid.\")\n\n        if self.bias_used:\n            phi = np.append(phi, np.ones((phi.shape[0], 1)), axis=1)\n\n        return phi\n\n    def _prune(self):\n        \"\"\"Remove basis functions based on alpha values.\"\"\"\n        keep_alpha = self.alpha_ < self.threshold_alpha\n\n        if not np.any(keep_alpha):\n            keep_alpha[0] = True\n            if self.bias_used:\n                keep_alpha[-1] = True\n\n        if self.bias_used:\n            if not keep_alpha[-1]:\n                self.bias_used = False\n            self.relevance_ = self.relevance_[keep_alpha[:-1]]\n        else:\n            self.relevance_ = self.relevance_[keep_alpha]\n\n        self.alpha_ = self.alpha_[keep_alpha]\n        self.alpha_old = self.alpha_old[keep_alpha]\n        self.gamma = self.gamma[keep_alpha]\n        self.phi = self.phi[:, keep_alpha]\n        self.sigma_ = self.sigma_[np.ix_(keep_alpha, keep_alpha)]\n        self.m_ = self.m_[keep_alpha]\n\n    def fit(self, X, y):\n        \"\"\"Fit the RVR to the training data.\"\"\"\n        X, y = check_X_y(X, y)\n\n        n_samples, n_features = X.shape\n\n        self.phi = self._apply_kernel(X, X)\n\n        n_basis_functions = self.phi.shape[1]\n\n        self.relevance_ = X\n        self.y = y\n\n        self.alpha_ = self.alpha * np.ones(n_basis_functions)\n        self.beta_ = self.beta\n\n        self.m_ = np.zeros(n_basis_functions)\n\n        self.alpha_old = self.alpha_\n\n        for i in range(self.n_iter):\n            self._posterior()\n\n            self.gamma = 1 - self.alpha_*np.diag(self.sigma_)\n            self.alpha_ = self.gamma/(self.m_ ** 2)\n\n            if not self.beta_fixed:\n                self.beta_ = (n_samples - np.sum(self.gamma))/(\n                    np.sum((y - np.dot(self.phi, self.m_)) ** 2))\n\n            self._prune()\n\n            if self.verbose:\n                print(\"Iteration: {}\".format(i))\n                print(\"Alpha: {}\".format(self.alpha_))\n                print(\"Beta: {}\".format(self.beta_))\n                print(\"Gamma: {}\".format(self.gamma))\n                print(\"m: {}\".format(self.m_))\n                print(\"Relevance Vectors: {}\".format(self.relevance_.shape[0]))\n                print()\n\n            delta = np.amax(np.absolute(self.alpha_ - self.alpha_old))\n\n            if delta < self.tol and i > 1:\n                break\n\n            self.alpha_old = self.alpha_\n\n        if self.bias_used:\n            self.bias = self.m_[-1]\n        else:\n            self.bias = None\n\n        return self\n\n\nclass RVR(BaseRVM, RegressorMixin):\n\n    \"\"\"Relevance Vector Machine Regression.\n\n    Implementation of Mike Tipping's Relevance Vector Machine for regression\n    using the scikit-learn API.\n    \"\"\"\n\n    def _posterior(self):\n        \"\"\"Compute the posterior distriubtion over weights.\"\"\"\n        i_s = np.diag(self.alpha_) + self.beta_ * np.dot(self.phi.T, self.phi)\n        self.sigma_ = np.linalg.inv(i_s)\n        self.m_ = self.beta_ * np.dot(self.sigma_, np.dot(self.phi.T, self.y))\n\n    def predict(self, X, eval_MSE=False):\n        \"\"\"Evaluate the RVR model at x.\"\"\"\n        phi = self._apply_kernel(X, self.relevance_)\n\n        y = np.dot(phi, self.m_)\n\n        if eval_MSE:\n            MSE = (1/self.beta_) + np.dot(phi, np.dot(self.sigma_, phi.T))\n            return y, MSE[:, 0]\n        else:\n            return y\n\n\nclass RVC(BaseRVM, ClassifierMixin):\n\n    \"\"\"Relevance Vector Machine Classification.\n\n    Implementation of Mike Tipping's Relevance Vector Machine for\n    classification using the scikit-learn API.\n    \"\"\"\n\n    def __init__(self, n_iter_posterior=50, **kwargs):\n        \"\"\"Copy params to object properties, no validation.\"\"\"\n        self.n_iter_posterior = n_iter_posterior\n        super(RVC, self).__init__(**kwargs)\n\n    def get_params(self, deep=True):\n        \"\"\"Return parameters as a dictionary.\"\"\"\n        params = super(RVC, self).get_params(deep=deep)\n        params['n_iter_posterior'] = self.n_iter_posterior\n        return params\n\n    def _classify(self, m, phi):\n        return expit(np.dot(phi, m))\n\n    def _log_posterior(self, m, alpha, phi, t):\n\n        y = self._classify(m, phi)\n\n        log_p = -1 * (np.sum(np.log(y[t == 1]), 0) +\n                      np.sum(np.log(1-y[t == 0]), 0))\n        log_p = log_p + 0.5*np.dot(m.T, np.dot(np.diag(alpha), m))\n\n        jacobian = np.dot(np.diag(alpha), m) - np.dot(phi.T, (t-y))\n\n        return log_p, jacobian\n\n    def _hessian(self, m, alpha, phi, t):\n        y = self._classify(m, phi)\n        B = np.diag(y*(1-y))\n        return np.diag(alpha) + np.dot(phi.T, np.dot(B, phi))\n\n    def _posterior(self):\n        result = minimize(\n            fun=self._log_posterior,\n            hess=self._hessian,\n            x0=self.m_,\n            args=(self.alpha_, self.phi, self.t),\n            method='Newton-CG',\n            jac=True,\n            options={\n                'maxiter': self.n_iter_posterior\n            }\n        )\n\n        self.m_ = result.x\n        self.sigma_ = np.linalg.inv(\n            self._hessian(self.m_, self.alpha_, self.phi, self.t)\n        )\n\n    def fit(self, X, y):\n        \"\"\"Check target values and fit model.\"\"\"\n        self.classes_ = np.unique(y)\n        n_classes = len(self.classes_)\n\n        if n_classes < 2:\n            raise ValueError(\"Need 2 or more classes.\")\n        elif n_classes == 2:\n            self.t = np.zeros(y.shape)\n            self.t[y == self.classes_[1]] = 1\n            return super(RVC, self).fit(X, self.t)\n        else:\n            self.multi_ = None\n            self.multi_ = OneVsOneClassifier(self)\n            self.multi_.fit(X, y)\n            return self\n\n    def predict_proba(self, X):\n        \"\"\"Return an array of class probabilities.\"\"\"\n        phi = self._apply_kernel(X, self.relevance_)\n        y = self._classify(self.m_, phi)\n        return np.column_stack((1-y, y))\n\n    def predict(self, X):\n        \"\"\"Return an array of classes for each input.\"\"\"\n        if len(self.classes_) == 2:\n            y = self.predict_proba(X)\n            res = np.empty(y.shape[0], dtype=self.classes_.dtype)\n            res[y[:, 1] <= 0.5] = self.classes_[0]\n            res[y[:, 1] >= 0.5] = self.classes_[1]\n            return res\n        else:\n            return self.multi_.predict(X)","metadata":{"execution":{"iopub.status.busy":"2023-11-10T13:41:55.248978Z","iopub.execute_input":"2023-11-10T13:41:55.249316Z","iopub.status.idle":"2023-11-10T13:41:55.307290Z","shell.execute_reply.started":"2023-11-10T13:41:55.249288Z","shell.execute_reply":"2023-11-10T13:41:55.306240Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"##################### PRE-PROCESSSS ##################\n\n\ndef preprocess(X_train: np.array, y_train: np.array, X_test: np.array, ids_train:np.array,ids_test:np.array, **kwargs):\n    X_train, X_test = impute_mv(X_train, X_test,kwargs['impute_data'])\n    print(\"Done with imputation\")\n#     x_mapped_to_isnan = np.isnan(X_train)\n#     x_mapped_to_isinf = np.isinf(X_train)\n#     for i in range(x_mapped_to_isnan.shape[0]):\n#         for j in range(x_mapped_to_isnan.shape[1]):\n#             if(x_mapped_to_isnan[i,j]==True):\n#                 print(f'anomaly isnan at:: {i},{j}')\n#     for i in range(x_mapped_to_isinf.shape[0]):\n#         for j in range(x_mapped_to_isinf.shape[1]):\n#             if(x_mapped_to_isinf[i,j]==True):\n#                 print(f'anomaly isinf at:: {i},{j}')\n\n#     print(x_mapped_to_isnan)\n    \n#     print(x_mapped_to_isinf)\n#     print(X_test)\n    # TODO\n\n#     X_train, X_test = scale_data(X_train, X_test,'min_max')\n#     print(\"Done with Scaling\")\n    X_train,y_train,ids_train = detect_remove_outliers(X_train,y_train, ids_train,kwargs['outlier_detection'],kwargs['contaminationRate'])\n    print(\"Done with outliers\")\n\n    X_train, X_test = select_features(X_train, y_train, X_test,kwargs['feature_selection'],featuresNo=kwargs['featuresNo'])\n    print(\"Done with Feature selection\")\n    \n    X_train, X_test = scale_data(X_train, X_test,kwargs['scale_data'])\n    print(\"Done with Scaling\")\n#     X_train, X_test = reduce_dim(X_train, X_test,kwargs['reduce_dim'])\n#     print(\"Done with reducing dim\")\n\n#     X_train, X_test = scale_data(X_train, X_test,kwargs['scale_data'])\n#     print(\"Done with Scaling\")\n    x_mapped_to_isnan = np.isnan(X_train)\n    x_mapped_to_isinf = np.isinf(X_train)\n    for i in range(x_mapped_to_isnan.shape[0]):\n        for j in range(x_mapped_to_isnan.shape[1]):\n            if(x_mapped_to_isnan[i,j]==True):\n                print(f'anomaly isnan at:: {i},{j}')\n    for i in range(x_mapped_to_isinf.shape[0]):\n        for j in range(x_mapped_to_isinf.shape[1]):\n            if(x_mapped_to_isinf[i,j]==True):\n                print(f'anomaly isinf at:: {i},{j}')\n\n    return X_train, y_train, X_test,ids_train,ids_test\n\n\ndef reduce_dim(X_train, X_test, method: str = 'PCA'):\n    if method == 'PCA':\n        reducer = PCA(n_components='mle', svd_solver='auto')\n\n    elif method == 'UMAP':\n        reducer = umap.UMAP()\n\n    else:\n        return X_train, X_test\n\n    X_train = reducer.fit_transform(X_train)\n    X_test = reducer.transform(X_test)\n    return X_train, X_test\n\n\ndef select_features(X_train: np.array, y_train: np.array, X_test: np.array, method:str = 'correlation', featuresNo=175):\n    if method == 'correlation':\n        X_train, X_test = remove_correlated(X_train, X_test)\n        # Select k best\n        fs = SelectKBest(score_func=f_regression, k=featuresNo)\n        X_train = fs.fit_transform(X_train, y_train)\n        X_test = fs.transform(X_test)\n    elif method=='kBest':\n        fs = SelectKBest(score_func=f_regression, k=featuresNo)\n        X_train = fs.fit_transform(X_train, y_train)\n        X_test = fs.transform(X_test)\n        \n    else:\n        raise Exception(f\"Select_features: {method} is not implemented\")\n        \n    # # Chi\n    # f_p_values = chi2(X_train, y_train)\n    # print(f_p_values)\n    return X_train, X_test\n\n\ndef remove_correlated(X_train: np.array, X_test: np.array):\n    # Constant features\n    var_threshold = VarianceThreshold(threshold=0)  # threshold = 0 for constant\n    var_threshold.fit_transform(X_train)\n    var_threshold.transform(X_test)\n\n    # Correlated\n    cor = corr2_coeff(X_train.T, X_train.T)\n    p = np.argwhere(np.triu(np.isclose(cor, 1), 1))\n    X_train = np.delete(X_train, p[:, 1], axis=1)\n    X_test = np.delete(X_test, p[:, 1], axis=1)\n    return X_train, X_test\n\n\ndef scale_data(X_train: np.array, X_test: np.array, method: str = 'min_max'):\n    if method == 'robust':\n        transformer = RobustScaler()\n    elif method == 'min_max':\n        transformer = MinMaxScaler()\n    elif method == 'NONE':\n        return X_train, X_test\n    elif method =='standard':\n        transformer = StandardScaler()\n    else:\n        raise Exception(f\"Scale: {method} is not implemented\")\n\n    X_train = transformer.fit_transform(X_train)\n    X_test = transformer.transform(X_test)\n    return X_train, X_test\n\n\ndef impute_mv(X_train: np.array, X_test: np.array, method: str = 'KNN'):\n    if method == 'median':\n        imp = SimpleImputer(missing_values=np.nan, strategy='median')\n    elif method == 'mean':\n        imp = SimpleImputer(missing_values=np.nan,strategy='mean')\n    elif method == 'KNN':\n        K=3\n        imp = KNNImputer(n_neighbors=K)\n    elif method == 'iterative': #aka mice\n        imp = IterativeImputer(estimator=BayesianRidge(), n_nearest_features=None, imputation_order='ascending')\n\n    else:\n        raise Exception(f\"Impute: {method} is not implemented\")\n\n    X_train = imp.fit_transform(X_train)\n    X_test = imp.fit_transform(X_test)\n\n    return X_train, X_test\n\n\ndef detect_remove_outliers(X_train: np.array, y_train: np.array, ids_train: np.array,method:str ='ECOD',contaminationRate=0.05):\n    # TODO\n    train_pred_indices = detect_outlier_obs(X_train, y_train, ids_train,method,contaminationRate)\n    #TODO:: UPDATE TO GET THE SPECIFIC INDICES BACK\n    X_train = (X_train[train_pred_indices])[0]\n    y_train = (y_train[train_pred_indices])[0]\n    ids_train = (ids_train[train_pred_indices])[0]\n    print(X_train.shape)\n    return X_train , y_train , ids_train\n\n\ndef detect_outlier_obs(X_train: np.array, y_train: np.array, X_test: np.array, method: str = 'ECOD',contaminationRate=0.05):\n    train_pred, test_pred = [], []\n    CONTAMINATION=contaminationRate\n    if method == 'ECOD':\n        ecod = ECOD(contamination=CONTAMINATION)\n        ecod.fit(X_train)\n        tmp_list = np.array(ecod.labels_).tolist()\n#         get labels and then adjust to have indices to remove outliers\n        s= len(tmp_list)\n        ones_removed=0\n        for i in range(s):\n            if(tmp_list[i-ones_removed]==0):\n                tmp_list[i-ones_removed]=i\n            elif(tmp_list[i-ones_removed]==1):\n                del tmp_list[i-ones_removed]\n                ones_removed+=1\n        train_pred.append(tmp_list)\n                \n    elif method == 'isolation_forest':\n        clf = IsolationForest(n_estimators=150, max_samples='auto', contamination=CONTAMINATION)\n        tmp_list = np.array(clf.fit_predict(X_train)).tolist()\n#         get labels and then adjust to have indices to remove outliers\n        s= len(tmp_list)\n        ones_removed=0\n        for i in range(s):\n            if(tmp_list[i-ones_removed]==1):\n                tmp_list[i-ones_removed]=i\n            elif(tmp_list[i-ones_removed]==-1):\n                del tmp_list[i-ones_removed]\n                ones_removed+=1\n        train_pred.append(tmp_list)\n\n    elif method==\"coresets\":\n        tree = CoresetTreeServiceDTC(optimized_for = 'cleaning')\n        tree = tree.build(X=X_train,y=y_train, chunk_size=-1)\n        result = tree.get_cleaning_samples(int(CONTAMINATION*X_train.shape[0]))\n        tree.remove_samples(result['idx'])\n        res = tree.get_cleaning_samples(X_train.shape[0])\n        train_pred.append(res['idx'])\n\n    else:\n        raise Exception(f\"Detect: {method} is not implemented\")\n    train_pred[0].sort() #in practice not needed but a neat thing to do \n    print(len(train_pred[0]))\n    return train_pred","metadata":{"execution":{"iopub.status.busy":"2023-11-10T13:41:55.309852Z","iopub.execute_input":"2023-11-10T13:41:55.310452Z","iopub.status.idle":"2023-11-10T13:41:55.349315Z","shell.execute_reply.started":"2023-11-10T13:41:55.310420Z","shell.execute_reply":"2023-11-10T13:41:55.348167Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# test\n# def read_data(X_train_path, y_train_path, X_test_path):\n#     X_train = np.genfromtxt(X_train_path, delimiter=\",\")\n#     y_train = np.genfromtxt(y_train_path, delimiter=\",\")\n#     X_test = np.genfromtxt(X_test_path, delimiter=\",\")\n#     return X_train, y_train, X_test\n\n# methods={\n#         \"impute_data\": ['median','mean','iterative','KNN'],\n#         \"outlier_detection\":['ECOD','isolation_forest','coresets'],\n#         \"feature_selection\":['correlation',],\n#         \"scale_data\":['robust','min_max','NONE'],\n#         \"reduce_dim\":['PCA','UMAP','NONE']\n#     }\n\n# for impute_method in methods['impute_data']:\n#     for outlier_detection_method in methods['outlier_detection']:\n#         for feature_selection_method in methods['feature_selection']:\n#             for scale_data_method in methods['scale_data']:\n#                 for reduce_dim_method in methods['reduce_dim']:\n#                     print(f'impute:: {impute_method},outlier:: {outlier_detection_method}, feature_select:: {feature_selection_method}, scale_data:: {scale_data_method},reduce_dim:: {reduce_dim_method}')\n#                     xTrainPath=\"/kaggle/input/aml-task1-allfiles/X_train.csv\"\n#                     yTrainPath=\"/kaggle/input/aml-task1-allfiles/y_train.csv\"\n#                     xTestPath=\"/kaggle/input/aml-task1-allfiles/X_test.csv\"\n#                     X_train, y_train, X_test = read_data(X_train_path=xTrainPath,\n#                                                          y_train_path=yTrainPath,\n#                                                          X_test_path=xTestPath)\n#                     ids_train, ids_test = X_train[1:, 0], X_test[1:, 0].astype(int)\n#                     X_train, y_train, X_test = X_train[1:, 1:], y_train[1:, 1:].ravel(), X_test[1:, 1:]\n#                     X_train, y_train, X_test,ids_train,ids_test = preprocess(X_train, y_train, X_test,ids_train, ids_test,\n#                             impute_data= impute_method,\n#                             outlier_detection=outlier_detection_method,\n#                             feature_selection=feature_selection_method,\n#                             scale_data=scale_data_method,\n#                             reduce_dim=reduce_dim_method)\n                    \n","metadata":{"execution":{"iopub.status.busy":"2023-11-10T13:41:55.350792Z","iopub.execute_input":"2023-11-10T13:41:55.351122Z","iopub.status.idle":"2023-11-10T13:41:55.369950Z","shell.execute_reply.started":"2023-11-10T13:41:55.351095Z","shell.execute_reply":"2023-11-10T13:41:55.369120Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"###################### Train ######################\n\n\nnp.random.seed(42)\n\n\ndef read_data(X_train_path, y_train_path, X_test_path):\n    X_train = np.genfromtxt(X_train_path, delimiter=\",\")\n    y_train = np.genfromtxt(y_train_path, delimiter=\",\")\n    X_test = np.genfromtxt(X_test_path, delimiter=\",\")\n    return X_train, y_train, X_test\n\n\n\ndef get_model(method: int = 3):\n    \n    if method == 0:\n        estimators = [\n            ('xgb', XGBRegressor(n_estimators=1000, max_depth=7, eta=0.1, colsample_bytree=0.8)),\n            ('extratree', ExtraTreesRegressor(n_estimators=1000, random_state=0)),\n#             ('adaboost', AdaBoostRegressor(n_estimators=1000, random_state=0)),\n            ('lgbm', LGBMRegressor()),\n#             ('svr_rbf', SVR(kernel='rbf')),\n#             ('mn', KNeighborsRegressor()),\n            ('rvm', RVR(alpha=1e-06)),\n            ('cat', CatBoostRegressor()),\n            ('gp', GaussianProcessRegressor(kernel=RationalQuadratic(alpha=0.5), random_state=42)),\n#             ('cubist', cubist.Cubist()),\n        ]\n        model = StackingRegressor(estimators=estimators,\n                                  final_estimator=RidgeCV())\n    elif method == 1:\n        estimators = [\n            # ('lr', RidgeCV()),\n            # ('lasso', Lasso(alpha=0.134694)),\n            # ('enet', ElasticNet(alpha=0.201, l1_ratio=0.005)),\n            # ('lm', LinearRegression()),\n            # ('kernel_ridge', KernelRidge(alpha=2.0, kernel='polynomial', degree=1, coef0=0.005)),\n            ('xgb', XGBRegressor(n_estimators=1000, max_depth=7, eta=0.1, colsample_bytree=0.8)),\n            ('extratree', ExtraTreesRegressor(n_estimators=1000, random_state=0)),\n            ('adaboost', AdaBoostRegressor(n_estimators=1000, random_state=0)),\n            # ('svr_lin', SVR(kernel='linear')),\n            # ['svr_rbf', SVR(kernel='rbf')],\n            ('mn', KNeighborsRegressor()),\n        ]\n        model = StackingRegressor(estimators=estimators, final_estimator=RandomForestRegressor(n_estimators=100, random_state=42))\n#TODO:: fix method2 error\n\n    elif method == 2:\n        estimators = [\n            ('xgb', XGBRegressor(n_estimators=1000, max_depth=7, eta=0.1, colsample_bytree=0.8)),\n            ('extratree', ExtraTreesRegressor(n_estimators=1000, random_state=0)),\n            ('adaboost', AdaBoostRegressor(n_estimators=1000, random_state=0)),\n            ('lgbm', LGBMRegressor()),\n            ('svr_rbf', SVR(kernel='rbf')),\n            ('mn', KNeighborsRegressor()),\n            ('rvm', RVR(alpha=1e-06)),\n            ('cat', CatBoostRegressor()),\n            ('gp', GaussianProcessRegressor(kernel=RationalQuadratic(alpha=0.5), random_state=42)),\n#             ('cubist', cubist.Cubist()),\n        ]\n        model = StackingRegressor(estimators=estimators,\n                                  final_estimator=RidgeCV())\n    elif method==3:\n        model = XGBRegressor(n_estimators=1000, max_depth=7, eta=0.1, colsample_bytree=0.8)\n    elif method==4:\n        model = ExtraTreesRegressor(n_estimators=1000, random_state=0)\n    elif method==5:\n        model = AdaBoostRegressor(n_estimators=1000, random_state=0)\n    elif method==6:\n        model = LGBMRegressor()\n    elif method==7:\n        model = SVR(kernel='rbf')\n    elif method==8:\n        model = KNeighborsRegressor()\n    elif method==9:\n        model = RVR(alpha=1e-06)\n    elif method==10:\n        model = CatBoostRegressor()\n    elif method==11:\n        model =  GaussianProcessRegressor(kernel=RationalQuadratic(alpha=0.5), random_state=42)\n    elif method==12:\n        model = cubist.Cubist()\n    else:\n        raise Exception(f\"Model: {method} is not implemented.\")\n\n    return model\n\ndef get_splits(X_train: np.array, nfolds: int = 10):\n    kf = KFold(n_splits=nfolds, random_state=42, shuffle=True)\n    return kf.split(X_train)\n# impute:: median, outlier:: isolation_forest, feature_select:: correlation, scale_data:: NONE,reduce_dim:: PCA\n\n\ndef main():\n#     methods={\n#         \"impute_data\": ['iterative','median','mean','KNN'],\n#         \"outlier_detection\":['coresets','ECOD','isolation_forest'],\n#         \"feature_selection\":['correlation'],\n#         \"featuresNo\":[100,150,200,250,300,350,400],\n#         \"scale_data\":['robust','min_max','standard','NONE'],\n#         \"reduce_dim\":['NONE','PCA','UMAP']\n#     }\n    \n    methods={\n        \"impute_data\": ['median'],\n        \"outlier_detection\":['isolation_forest'],\n#         \"contamination_rate\":[0.01,0.02,0.05,0.07,0.1,0.15,0.2],\n        \"contamination_rate\":[0.05],\n        \"feature_selection\":['kBest'],\n#         \"featuresNo\":[100,150,200,250,300,350,400],\n        \"featuresNo\":[200],\n        \"scale_data\":['standard'],\n        \"reduce_dim\":['NONE']\n    }\n    model_version = 0\n    only_preprocess = False\n    for impute_method in methods['impute_data']:\n        for outlier_detection_method in methods['outlier_detection']:\n            for feature_selection_method in methods['feature_selection']:\n                for featureNo in methods['featuresNo']:\n                    for contaminationRate in methods['contamination_rate']:\n                        for scale_data_method in methods['scale_data']:\n                            for reduce_dim_method in methods['reduce_dim']:\n                                print(f'impute:: {impute_method}, outlier:: {outlier_detection_method}_{contaminationRate}, feature_select:: {feature_selection_method}_{featureNo}, scale_data:: {scale_data_method}, reduce_dim:: {reduce_dim_method}')                        \n                                if scale_data_method == 'NONE'and reduce_dim_method == 'PCA':\n                                    print(\"Skipping the combination of scale_data method:: NONE, reduce_dim_method:: PCA due to joint error\")\n                                    continue\n                                xTrainPath=\"/kaggle/input/aml-task1-allfiles/X_train.csv\"\n                                yTrainPath=\"/kaggle/input/aml-task1-allfiles/y_train.csv\"\n                                xTestPath=\"/kaggle/input/aml-task1-allfiles/X_test.csv\"\n                                X_train, y_train, X_test = read_data(X_train_path=xTrainPath,\n                                                                     y_train_path=yTrainPath,\n                                                                     X_test_path=xTestPath)\n                                ids_train, ids_test = X_train[1:, 0], X_test[1:, 0].astype(int)\n                                X_train, y_train, X_test = X_train[1:, 1:], y_train[1:, 1:].ravel(), X_test[1:, 1:]\n                                X_train, y_train, X_test,ids_train,ids_test = preprocess(X_train, y_train, X_test,ids_train, ids_test,\n                                    impute_data= impute_method,\n                                    outlier_detection=outlier_detection_method,\n                                    contaminationRate=contaminationRate,\n                                    feature_selection=feature_selection_method,\n                                    featuresNo=featureNo,\n                                    scale_data=scale_data_method,\n                                    reduce_dim=reduce_dim_method)\n                                print(\"Preprocessed.\")\n                                if only_preprocess:\n                                    path_out_X_train_preprocessed='/kaggle/working/'+'X_train_preprocessed_'+impute_method+'_'+outlier_detection_method+'_'+str(contaminationRate)+'_'+feature_selection_method+'_'+str(featureNo)+'_'+scale_data_method+'_'+reduce_dim_method+'.csv'\n                                    path_out_X_test_preprocessed='/kaggle/working/'+'X_test_preprocessed_'+impute_method+'_'+outlier_detection_method+'_'+str(contaminationRate)+'_'+feature_selection_method+'_'+str(featureNo)+'_'+scale_data_method+'_'+reduce_dim_method+'.csv'\n                                    path_out_y_train_preprocessed='/kaggle/working/'+'y_train_preprocessed_'+impute_method+'_'+outlier_detection_method+'_'+str(contaminationRate)+'_'+feature_selection_method+'_'+str(featureNo)+'_'+scale_data_method+'_'+reduce_dim_method+'.csv'\n                                    np.savetxt(path_out_X_train_preprocessed, X_train, delimiter=\",\", comments='')\n                                    np.savetxt(path_out_X_test_preprocessed, X_test,  delimiter=\",\",  comments='')\n                                    np.savetxt(path_out_y_train_preprocessed, y_train,  delimiter=\",\", header=\"id,y\", comments='')\n                                else:\n                                    model = get_model(model_version)\n\n                                    nfolds = 10\n                                    splits = get_splits(X_train, nfolds)\n\n                                    print(\"\\nModels and folds.\")\n\n                                    r2 = 0\n                                    for i, (train_index, test_index) in enumerate(splits):\n                                        model.fit(X_train[train_index], y_train[train_index])\n                                        pred = model.predict(X_train[test_index])\n                                        score = r2_score(y_train[test_index], pred)\n                                        r2 += score\n\n                                        print(f\"Fold {i} R2 score: {score}\")\n\n                                    print(f\"\\nAvg R2: {r2 / nfolds}\")\n\n                                    print(\"\\nTrained.\")\n\n                                    model.fit(X_train, y_train)\n                                    pred = model.predict(X_test)\n                                    res = np.column_stack((ids_test, pred))\n                                    path_out='/kaggle/working/'+'out_'+impute_method+'_'+outlier_detection_method+'_'+str(contaminationRate)+'_'+feature_selection_method+'_'+str(featureNo)+'_'+scale_data_method+'_'+reduce_dim_method+'.csv'\n                                    np.savetxt(path_out, res, fmt=['%1i', '%1.4f'], delimiter=\",\", header=\"id,y\", comments='')\n\n                            #     X_train, y_train, X_test = preprocess(X_train, y_train, X_test)\n    #                             try:\n    #                                 X_train, y_train, X_test,ids_train,ids_test = preprocess(X_train, y_train, X_test,ids_train, ids_test,\n    #                                     impute_data= impute_method,\n    #                                     outlier_detection=outlier_detection_method,\n    #                                     feature_selection=feature_selection_method,\n    #                                     featuresNo=featureNo,\n    #                                     scale_data=scale_data_method,\n    #                                     reduce_dim=reduce_dim_method)\n    #                                 print(\"Preprocessed.\")\n    #                                 if only_preprocess:\n    #                                     path_out_X_train_preprocessed='/kaggle/working/'+'X_train_preprocessed_'+impute_method+'_'+outlier_detection_method+'_'+feature_selection_method+'_'+str(featureNo)+'_'+scale_data_method+'_'+reduce_dim_method+'.csv'\n    #                                     path_out_X_test_preprocessed='/kaggle/working/'+'X_test_preprocessed_'+impute_method+'_'+outlier_detection_method+'_'+feature_selection_method+'_'+str(featureNo)+'_'+scale_data_method+'_'+reduce_dim_method+'.csv'\n    #                                     path_out_y_train_preprocessed='/kaggle/working/'+'y_train_preprocessed_'+impute_method+'_'+outlier_detection_method+'_'+feature_selection_method+'_'+str(featureNo)+'_'+scale_data_method+'_'+reduce_dim_method+'.csv'\n    #                                     np.savetxt(path_out_X_train_preprocessed, X_train, delimiter=\",\", comments='')\n    #                                     np.savetxt(path_out_X_test_preprocessed, X_test,  delimiter=\",\",  comments='')\n    #                                     np.savetxt(path_out_y_train_preprocessed, y_train,  delimiter=\",\", header=\"id,y\", comments='')\n    #                                 else:\n    #                                     model = get_model()\n\n    #                                     nfolds = 10\n    #                                     splits = get_splits(X_train, nfolds)\n\n    #                                     print(\"\\nModels and folds.\")\n\n    #                                     r2 = 0\n    #                                     for i, (train_index, test_index) in enumerate(splits):\n    #                                         model.fit(X_train[train_index], y_train[train_index])\n    #                                         pred = model.predict(X_train[test_index])\n    #                                         score = r2_score(y_train[test_index], pred)\n    #                                         r2 += score\n\n    #                                         print(f\"Fold {i} R2 score: {score}\")\n\n    #                                     print(f\"\\nAvg R2: {r2 / nfolds}\")\n\n    #                                     print(\"\\nTrained.\")\n\n    #                                     model.fit(X_train, y_train)\n    #                                     pred = model.predict(X_test)\n    #                                     res = np.column_stack((ids_test, pred))\n    #                                     path_out='/kaggle/working/'+'out_'+impute_method+'_'+outlier_detection_method+'_'+feature_selection_method+'_'+str(featureNo)+'_'+scale_data_method+'_'+reduce_dim_method+'.csv'\n    #                                     np.savetxt(path_out, res, fmt=['%1i', '%1.4f'], delimiter=\",\", header=\"id,y\", comments='')\n\n    #                             except ValueError:\n    #                                 print(f'Error in::impute:: {impute_method}, outlier:: {outlier_detection_method}, feature_select:: {feature_selection_method}, scale_data:: {scale_data_method}, reduce_dim:: {reduce_dim_method}')                                       \n    #                                 pass\n\n\nif __name__ == \"__main__\":\n    main()","metadata":{"execution":{"iopub.status.busy":"2023-11-10T13:49:12.314172Z","iopub.execute_input":"2023-11-10T13:49:12.314582Z","iopub.status.idle":"2023-11-10T14:00:04.225022Z","shell.execute_reply.started":"2023-11-10T13:49:12.314549Z","shell.execute_reply":"2023-11-10T14:00:04.223129Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"impute:: median, outlier:: isolation_forest_0.05, feature_select:: kBest_200, scale_data:: standard, reduce_dim:: NONE\nDone with imputation\n1151\n(1151, 832)\nDone with outliers\nDone with Feature selection\nDone with Scaling\nPreprocessed.\n\nModels and folds.\nFold 0 R2 score: 0.5361839764316751\nFold 1 R2 score: 0.5500909635579436\nFold 2 R2 score: 0.5562975624410124\nFold 3 R2 score: 0.4666050768988863\nFold 4 R2 score: 0.40604863590038753\nFold 5 R2 score: 0.6651928456446853\nFold 6 R2 score: 0.6389019791951429\nFold 7 R2 score: 0.4452669957933376\nFold 8 R2 score: 0.5705946246504346\nFold 9 R2 score: 0.5608811687756635\n\nAvg R2: 0.5396063829289168\n\nTrained.\n","output_type":"stream"}]}]}