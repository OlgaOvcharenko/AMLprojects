{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6964300,"sourceType":"datasetVersion","datasetId":4000840}],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-16T17:09:31.652588Z","iopub.execute_input":"2023-11-16T17:09:31.653059Z","iopub.status.idle":"2023-11-16T17:09:31.663938Z","shell.execute_reply.started":"2023-11-16T17:09:31.653024Z","shell.execute_reply":"2023-11-16T17:09:31.662211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install heartpy\n!pip install pyhrv\n!pip install neurokit2\n!pip install statsmodels\n!pip install biosppy\n!pip install tqdm\n!pip install phate\n!pip install pyod\n!pip install dataheroes","metadata":{"execution":{"iopub.status.busy":"2023-11-16T18:51:28.131494Z","iopub.execute_input":"2023-11-16T18:51:28.131873Z","iopub.status.idle":"2023-11-16T18:52:54.929512Z","shell.execute_reply.started":"2023-11-16T18:51:28.131843Z","shell.execute_reply":"2023-11-16T18:52:54.928328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport heartpy as hp\nfrom heartpy.analysis import calc_fd_measures\nimport scipy\nfrom scipy.stats import kurtosis\nfrom scipy.stats import skew\nfrom heartpy.analysis import clean_rr_intervals\nfrom heartpy.analysis import calc_ts_measures\nimport pyhrv.tools as tools\nfrom pyhrv.hrv import hrv\nimport neurokit2 as nk\nimport numpy as np\nimport statsmodels.api as sm\nfrom scipy.signal.signaltools import wiener\nimport biosppy\nfrom tqdm import tqdm\nimport tensorflow as tf\nfrom tensorflow.keras.utils import pad_sequences\nimport pywt\n\n\nclass Extractor:\n    def __init__(self, x):\n        self.X = x\n        \n    def extract(self):\n        vals = []\n        for index in tqdm(range(0, self.X.shape[0])):\n            maxlen = self.X.shape[1] - 9669\n            #padded_row = pad_sequences(self.X.loc[index, :], maxlen=maxlen, dtype=float, padding='post', truncating = 'post', value=self.X.loc[index, maxlen - 1])\n            row = self.X.loc[index, :]\n            res = self._extract_one(row)\n            vals.append(res)\n\n        res = pd.DataFrame.from_records(vals)\n\n        return res\n        \n    def _extract_one(self, signal):\n        cur_row = {}\n\n        # Automatically process the (raw) ECG signal\n        cleaned_ecg = nk.ecg_clean(signal, sampling_rate=300, method='biosppy')\n        cleaned_ecg = hp.remove_baseline_wander(cleaned_ecg, 300)\n        cleaned_ecg = nk.signal_detrend(cleaned_ecg)\n\n        rpeaks = biosppy.signals.ecg.hamilton_segmenter(cleaned_ecg, 300)\n        rpeaks = biosppy.signals.ecg.correct_rpeaks(cleaned_ecg, rpeaks[0], 300.0)[0]\n\n        r_vals = [cleaned_ecg[r] for r in rpeaks if not np.isnan(r)]\n        for i, val in enumerate(r_vals):\n            cur_row[f\"r{i}\"] = val\n        \n        # Delineate the ECG signal\n        _, waves_dwt = nk.ecg_delineate(cleaned_ecg, sampling_rate=300, method=\"dwt\")\n        p_onsets = waves_dwt[\"ECG_P_Onsets\"]\n        p_offsets = waves_dwt[\"ECG_P_Offsets\"]\n        p_peaks = waves_dwt[\"ECG_P_Peaks\"]\n\n        s_peaks = waves_dwt[\"ECG_S_Peaks\"]\n\n        q_peaks = waves_dwt[\"ECG_Q_Peaks\"]\n\n        r_onsets = waves_dwt[\"ECG_R_Onsets\"]\n        r_offsets = waves_dwt[\"ECG_R_Offsets\"]\n\n        t_onsets = waves_dwt[\"ECG_T_Onsets\"]\n        t_offsets = waves_dwt[\"ECG_T_Offsets\"]\n        t_peaks = waves_dwt[\"ECG_T_Peaks\"]\n\n        n_heartbeats = np.size(rpeaks)\n\n        #rr_distance\n        rr = (rpeaks[1:] - rpeaks[:n_heartbeats - 1])  # rr-rate in seconds\n        cur_row[\"mean_rr\"] = np.nanmean(rr)\n        # cur_row[\"median_rr\"] = np.nanmedian(rr)\n        cur_row[\"var_rr\"] = np.nanvar(rr)\n        cur_row[\"max_rr\"] = np.nanmax(rr)\n        cur_row[\"min_rr\"] = np.nanmin(rr)\n        cur_row[\"skew_rr\"] = skew(rr)\n        cur_row[\"kurtosis_rr\"] = kurtosis(rr)\n\n        #r_amplitude\n        r_amplitude = cleaned_ecg[rpeaks]\n\n        cur_row[\"mean_r\"] = np.nanmean(r_amplitude)\n        cur_row[\"median_r\"] = np.nanmedian(r_amplitude)\n        cur_row[\"var_r\"] = np.nanvar(r_amplitude)\n        cur_row[\"max_r\"] = np.nanmax(r_amplitude)\n        cur_row[\"min_r\"] = np.nanmin(r_amplitude)\n\n        cur_row[\"skew_r\"] = skew(r_amplitude)\n        cur_row[\"kurtosis_r\"] = kurtosis(r_amplitude)\n\n        # q_amplitude\n        q_amplitude = q_peaks\n        cur_row[\"mean_q\"] = np.nanmean(q_amplitude)\n        cur_row[\"median_q\"] = np.nanmedian(q_amplitude)\n        cur_row[\"var_q\"] = np.nanvar(q_amplitude)\n        cur_row[\"max_q\"] = np.nanmax(q_amplitude)\n        cur_row[\"min_q\"] = np.nanmin(q_amplitude)\n        cur_row[\"skew_q\"] = skew(q_amplitude)\n        cur_row[\"kurtosis_q\"] = kurtosis(q_amplitude)\n\n        # t_amplitude\n        t_amplitude = t_peaks\n        cur_row[\"mean_q\"] = np.nanmean(t_amplitude)\n        cur_row[\"median_q\"] = np.nanmedian(t_amplitude)\n        cur_row[\"var_t\"] = np.nanvar(t_amplitude)\n        cur_row[\"max_t\"] = np.nanmax(t_amplitude)\n        cur_row[\"min_t\"] = np.nanmin(t_amplitude)\n        cur_row[\"skew_t\"] = skew(t_amplitude)\n        cur_row[\"kurtosis_t\"] = kurtosis(t_amplitude)\n\n        # s_amplitude\n        s_amplitude = s_peaks\n        cur_row[\"mean_q\"] = np.nanmean(s_amplitude)\n        cur_row[\"median_q\"] = np.nanmedian(s_amplitude)\n        cur_row[\"var_s\"] = np.nanvar(s_amplitude)\n        cur_row[\"max_s\"] = np.nanmax(s_amplitude)\n        cur_row[\"min_s\"] = np.nanmin(s_amplitude)\n        cur_row[\"skew_s\"] = skew(s_amplitude)\n        cur_row[\"kurtosis_s\"] = kurtosis(s_amplitude)\n\n        #qrs_duration\n        qrs_duration = [(b - a) for a, b in zip(r_onsets, r_offsets)]\n        cur_row[\"mean_qrs\"] = np.nanmean(qrs_duration)\n        cur_row[\"median_qrs\"] = np.nanmedian(qrs_duration)\n        cur_row[\"var_qrs\"] = np.nanvar(qrs_duration)\n        cur_row[\"max_qrs\"] = np.nanmax(qrs_duration)\n        cur_row[\"min_qrs\"] = np.nanmin(qrs_duration)\n        cur_row[\"skew_qrs\"] = skew(qrs_duration)\n        cur_row[\"kurtosis_qrs\"] = kurtosis(qrs_duration)\n\n        #hrv metrics\n        hrv_time = nk.hrv_time(rpeaks, sampling_rate=300, show=False)\n        cur_row[\"HRV_IQRNN\"] = hrv_time[\"HRV_IQRNN\"].iloc[0]\n        cur_row[\"HRV_HTI\"] = hrv_time[\"HRV_HTI\"].iloc[0]\n        cur_row[\"HRV_pNN50\"] = hrv_time[\"HRV_pNN50\"].iloc[0]\n\n        # cur_row[\"HRV_SDNN\"] = hrv_time[\"HRV_SDNN\"].iloc[0]\n        # cur_row[\"HRV_RMSSD\"] = hrv_time[\"HRV_RMSSD\"].iloc[0]\n        # cur_row[\"HRV_SDSD\"] = hrv_time[\"HRV_SDSD\"].iloc[0]\n        # cur_row[\"HRV_CVNN\"] = hrv_time[\"HRV_CVNN\"].iloc[0]\n        # cur_row[\"HRV_MedianNN\"] = hrv_time[\"HRV_MedianNN\"].iloc[0]\n        # cur_row[\"HRV_pNN50\"] = hrv_time[\"HRV_pNN50\"].iloc[0]\n        # cur_row[\"HRV_pNN20\"] = hrv_time[\"HRV_pNN20\"].iloc[0]\n        # cur_row[\"HRV_TINN\"] = hrv_time[\"HRV_TINN\"].iloc[0]\n        \n        # FFT\n        fft = scipy.fft.fft(cleaned_ecg)\n        S = np.abs(fft)/len(cleaned_ecg) #careful, gives large numbers If small numbers are desired then /len(cleaned_ecg)\n        S = S[:int(len(S)/2)]\n        cur_row[\"fft_max\"] = np.max(S)\n        cur_row[\"fft_sum\"] = np.sum(S)\n        cur_row[\"fft_mean\"] = np.mean(S)\n        cur_row[\"fft_var\"] = np.var(S)\n        cur_row[\"fft_peak\"] = np.max(S)\n        cur_row[\"fft_skew\"] = skew(S)\n        cur_row[\"fft_kurtosis\"] = kurtosis(S)\n        \n        # Wavelet\n        fs = len(cleaned_ecg)\n        dist = np.nanmax(rr)\n        scales = range(1, 10*dist) #change dist coeff to change number of wavelets, 10 runs in 6 min\n        waveletname = 'morl'\n        coeff, freq = pywt.cwt(cleaned_ecg, scales, waveletname, 1)\n        energies = np.sum(np.square(np.abs(coeff)), axis=-1)\n        \n        for i, val in enumerate(energies):\n            cur_row[f\"wvt_energy_{i}\"] = val\n    \n        return cur_row\n","metadata":{"execution":{"iopub.status.busy":"2023-11-16T18:52:54.931481Z","iopub.execute_input":"2023-11-16T18:52:54.931775Z","iopub.status.idle":"2023-11-16T18:52:54.957968Z","shell.execute_reply.started":"2023-11-16T18:52:54.931750Z","shell.execute_reply":"2023-11-16T18:52:54.956951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport phate\nimport umap\nfrom matplotlib import pyplot as plt\nfrom mlxtend.plotting.pca_correlation_graph import corr2_coeff\nfrom pyod.models.ecod import ECOD\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import BayesianRidge, LassoCV, Lasso, LogisticRegression\nfrom sklearn.preprocessing import RobustScaler, MinMaxScaler, PolynomialFeatures, StandardScaler\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.feature_selection import f_regression, SelectKBest, chi2, VarianceThreshold, RFE\nfrom dataheroes import CoresetTreeServiceDTC\nfrom sklearn.linear_model import LinearRegression\nfrom scipy.stats import f_oneway\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.covariance import EllipticEnvelope\n\nimport pandas as pd\npd.DataFrame.iteritems = pd.DataFrame.items\n\n\ndef preprocess(X_train: np.array, y_train: np.array, X_test: np.array, drop_r: bool):\n    if drop_r:\n        X_train.drop([f\"r{r}\" for r in range(0, 160)], axis=1, inplace=True)\n\n    X_train.dropna(axis=1, how='all', inplace=True)\n    X_test = X_test[X_train.columns]\n\n    X_train.replace(np.inf,np.nan,inplace=True)\n    X_test.replace(np.inf,np.nan,inplace=True)\n\n    X_train, X_test = impute_mv(X_train, X_test, 'median')\n    X_train, X_test = select_features(X_train, y_train, X_test)\n    X_train, X_test = scale_data(X_train, X_test, 'standard')\n\n    return X_train, y_train, X_test\n\n\ndef make_polynomial(X_train: np.array, y_train: np.array, X_test: np.array, degree: int = 2):\n    if degree > 2:\n        raise Exception(\"make_polynomial: Insane degree.\")\n\n    poly = PolynomialFeatures(2)\n    X_train = poly.fit_transform(X_train, y_train)\n    X_test = poly.transform(X_test)\n\n    print(X_train.shape)\n    return X_train, X_test\n\n\ndef select_features(X_train: np.array, y_train: np.array, X_test: np.array):\n    print(X_train.shape)\n    X_train, X_test = remove_correlated(X_train, X_test)\n\n    # fs = SelectKBest(score_func=f_regression, k=200)\n    # X_train = fs.fit_transform(X_train, y_train.ravel())\n    # X_test = fs.transform(X_test)\n    # X_train, X_test = recursive_elemination(X_train, y_train, X_test)\n\n    print(X_train.shape)\n    return X_train, X_test\n\n\ndef recursive_elemination(X_train: np.array, y_train: np.array, X_test: np.array):\n    model = LogisticRegression()\n    rfe = RFE(model)\n\n    X_train = rfe.fit_transform(X_train, y_train)\n    X_test = rfe.transform(X_test)\n\n    return X_train, X_test\n\ndef remove_correlated(X_train: np.array, X_test: np.array):\n    # Constant features\n    var_threshold = VarianceThreshold(threshold=0.0)  # TODO threshold = 0 for constant\n    var_threshold.fit_transform(X_train)\n    var_threshold.transform(X_test)\n\n    # Correlated\n    cor = corr2_coeff(X_train.T, X_train.T)\n    p = np.argwhere(np.triu(np.isclose(cor, 1), 1))\n    X_train = np.delete(X_train, p[:, 1], axis=1)\n    X_test = np.delete(X_test, p[:, 1], axis=1)\n    return X_train, X_test\n\n\ndef scale_data(X_train: np.array, X_test: np.array, method: str = 'min_max'):\n    if method == 'robust':\n        transformer = RobustScaler()\n    elif method == 'min_max':\n        transformer = MinMaxScaler()\n    elif method == 'standard':\n        transformer = StandardScaler()\n    else:\n        raise Exception(f\"Scale: {method} is not implemented\")\n\n    X_train = transformer.fit_transform(X_train)\n    X_test = transformer.transform(X_test)\n    return X_train, X_test\n\n\ndef impute_mv(X_train: np.array, X_test: np.array, method: str = 'iterative'):\n    if method == 'median':\n        imp = SimpleImputer(missing_values=np.nan, strategy='median')\n\n    elif method == 'mean':\n        imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n\n    elif method == 'iterative':\n        imp = IterativeImputer(estimator=BayesianRidge(), n_nearest_features=None, imputation_order='ascending')\n\n    else:\n        raise Exception(f\"Impute: {method} is not implemented\")\n\n    X_train = imp.fit_transform(X_train)\n    X_test = imp.transform(X_test)\n\n    return X_train, X_test\n\n\ndef detect_remove_outliers(X_train: np.array, y_train: np.array, X_test: np.array):\n    train_pred1 = detect_outlier_obs(X_train, y_train, 'coresets')\n    train_pred2 = detect_outlier_obs(X_train, X_test, 'ECOD')\n    train_pred3 = detect_outlier_obs(X_train, X_test, 'elliptic')\n    train_pred4 = detect_outlier_obs(X_train, X_test, 'local_factor')\n\n    train_pred = train_pred1 + train_pred2 + train_pred3 + train_pred4\n    train_pred = train_pred > 1\n\n    print(X_train.shape)\n    X_train = X_train[train_pred]\n    print(X_train.shape)\n    y_train = y_train[train_pred]\n\n    return X_train, y_train, X_test\n\n\ndef detect_outlier_obs(X_train: np.array, y_train: np.array, method: str = 'elliptic'):\n    train_pred, test_pred = [], []\n    if method == 'ECOD':\n        ecod = ECOD(contamination=0.03)\n        ecod.fit(X_train)\n        train_pred = np.array(ecod.labels_) == 0\n\n    elif method == 'isolation_forest':\n        for i in range(X_train.shape[1]):\n            clf = IsolationForest(n_estimators=150, max_samples='auto', contamination=float(0.03))\n            y_train_pred = np.array(clf.fit_predict(X_train[:, i].reshape(-1, 1))) == 1\n            train_pred.append(y_train_pred)\n            # y_test_pred = np.array(clf.predict(X_test[:, i].reshape(-1, 1))) == 1\n            # test_pred.append(y_test_pred)\n\n        train_pred = np.array(train_pred).sum(axis=1)\n        # test_pred = np.array(test_pred).sum(axis=1)\n\n    elif method == \"coresets\":\n        tree = CoresetTreeServiceDTC(optimized_for='cleaning')\n        tree = tree.build(X=X_train, y=y_train, chunk_size=-1)\n        result = tree.get_cleaning_samples(20)\n        tree.remove_samples(result['idx'])\n        res = tree.get_cleaning_samples(1212)\n        # print(res['idx'].shape)\n        # print(res[\"idx\"])\n        train_pred = np.full((X_train.shape[0],), False)\n        train_pred[res[\"idx\"]] = True\n\n    elif method == \"local_factor\":\n        lo = LocalOutlierFactor(n_neighbors=2)\n        res = lo.fit_predict(X_train)\n        train_pred = np.array(res) == 0\n\n    elif method == \"elliptic\":\n        ee = EllipticEnvelope(random_state=42)\n        res = ee.fit_predict(X_train, y_train)\n        train_pred = np.array(res) == 0\n\n    else:\n        raise Exception(f\"Detect: {method} is not implemented\")\n\n    return train_pred.astype(int)","metadata":{"execution":{"iopub.status.busy":"2023-11-16T17:11:03.860769Z","iopub.execute_input":"2023-11-16T17:11:03.861076Z","iopub.status.idle":"2023-11-16T17:11:03.895913Z","shell.execute_reply.started":"2023-11-16T17:11:03.861052Z","shell.execute_reply":"2023-11-16T17:11:03.894434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import f1_score\nfrom tqdm import tqdm\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.ensemble import RandomForestClassifier, StackingClassifier\nimport lightgbm as lgb\nimport tensorflow.keras.layers as layers\n\n\ndef read_data(X_train_path, y_train_path, X_test_path, extract_data):  \n    X_train = pd.read_csv(X_train_path)\n    X_test = pd.read_csv(X_test_path)\n    y_train = pd.read_csv(y_train_path).iloc[:,1]\n\n    if extract_data:\n        train_ids, test_ids = X_train.iloc[:, 0], X_test.iloc[:, 0]\n        X_train, X_test = X_train.iloc[:,1:], X_test.iloc[:,1:]\n    else:\n        train_ids, test_ids = pd.DataFrame(list(range(0, X_train.shape[0]))), pd.DataFrame(list(range(0, X_test.shape[0])))\n\n    return X_train, y_train, train_ids, X_test, test_ids\n\n\ndef get_splits(X_train: np.array, y_train: np.array, nfolds: int = 10):\n    kf = StratifiedKFold(n_splits=nfolds, random_state=42, shuffle=True)\n    return kf.split(X_train, y_train)\n\n\ndef get_model(method: int = 3):\n    if method == 1:\n        model = XGBClassifier()\n\n    elif method == 2:\n        model = CatBoostClassifier(iterations=1000, learning_rate=0.01, logging_level='Silent')\n\n    elif method == 3:\n        estimators = [ \n            ('cb', CatBoostClassifier(iterations=1000, learning_rate=0.01, logging_level='Silent')),\n            ('xgb', XGBClassifier(random_state=42)),\n            # ('lgbm', lgb.LGBMClassifier(random_state=42))\n        ]\n    \n        model = StackingClassifier(estimators=estimators, final_estimator=CatBoostClassifier(iterations=1000, learning_rate=0.01, logging_level='Silent'))\n    elif method == 4:\n        model = tf.keras.Sequential([keras.Input(shape=(X.shape[1],)),\n                                    layers.Dense(2000, activation='relu'),\n                                    layers.Dense(1250, activation='relu'),\n                                    layers.Dense(500, activation='relu'),\n                                    layers.Dense(250, activation='relu'),\n                                    layers.Dense(3, activation='relu')])\n        model.compile(loss=keras.losses.BinaryCrossentropy(from_logits=True), \n                      optimizer=tf.keras.optimizers.Adam(learning_rate=0.001)) #Adam, SGD or adamax\n\n    return model\n\n\ndef main():\n    extract_data = True\n    nn = False\n    method = 3\n    if nn:\n        method = 4\n        \n    # read data\n    if extract_data:\n        X_train_path, y_train_path, X_test_path = \"/kaggle/input/aml-ecg/X_train.csv\", \"/kaggle/input/aml-ecg/y_train.csv\", \"/kaggle/input/aml-ecg/X_test.csv\"\n\n    else:\n        X_train_path, y_train_path, X_test_path = \"data/train_feat.csv\", \"data/y_train.csv\", \"data/test_feat.csv\"\n\n    X_train, y_train, train_ids, X_test, test_ids = read_data(X_train_path, y_train_path, X_test_path, extract_data)\n\n    # extract\n    if extract_data:\n        extr = Extractor(X_train)\n        train_feat = extr.extract()\n        X_train = train_feat\n        train_feat.to_csv(\"train_feat.csv\",index=False)\n\n        extr = Extractor(X_test)\n        test_feat = extr.extract()\n        X_test = test_feat\n        test_feat.to_csv(\"test_feat.csv\",index=False)\n    \n    print(\"Extracted / read data.\")\n\n    X_train, y_train, X_test = preprocess(X_train, y_train, X_test, drop_r=False)\n\n    print(\"Preprocessed.\")\n    \n    nfolds = 5\n    splits = get_splits(X_train, y_train, nfolds)\n    \n    model = get_model(method)\n    f1_scores = 0\n    for i, (train_index, test_index) in enumerate(splits):\n        model = get_model(method)\n        if nn:\n            model.fit(X_train[train_index], y_train[train_index], epochs=75)\n        else:\n            model.fit(X_train[train_index], y_train[train_index])\n\n        pred = model.predict(X_train[test_index])\n\n        score = f1_score(y_train[test_index], pred, average=\"micro\")\n\n        print(f\"Fold {i}: score {score}\")\n        f1_scores += score\n\n    print(f\"Avg F1: {f1_scores / nfolds}\")\n\n    model_full = get_model(method)\n    if nn:\n        model_full.fit(X_train,y_train, epochs=75)\n    else:\n        model_full.fit(X_train,y_train)\n        \n    \n    res = model_full.predict(X_test)\n\n    out = pd.DataFrame()\n    out[\"id\"] = test_ids.iloc[:, 0]\n    out[\"y\"] = res\n\n    out.to_csv(\"data/out.csv\", index=False)\n\n\nmain()","metadata":{"execution":{"iopub.status.busy":"2023-11-16T17:11:03.898352Z","iopub.execute_input":"2023-11-16T17:11:03.898888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}